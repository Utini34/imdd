{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis on Reviews\n",
    "\n",
    "In this project, we will use machine learning algorithms and deep learning models with different word embedding approaches for predicting the sentiment of reviews. We will use the following models and compare their performance on different datasets: LSTM, CNN, T5, LR, SVM, and RFT.\n",
    "\n",
    "Data sources\n",
    "* IMBD dataset\n",
    "  * https://huggingface.co/datasets/imdb\n",
    "* Word embedding\n",
    "  * GloVe - https://nlp.stanford.edu/projects/glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "In this section, we will set up all the necessary parameters and import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "from lstm import LSTMSentiment\n",
    "from cnn import CNNSentiment\n",
    "from t5_model import T5Sentiment\n",
    "from lr import LRSentiment\n",
    "from svm import SVMSentiment\n",
    "from rft import RFTSentiment\n",
    "from utils import load_data, preprocess_data, create_dataloader, load_glove_embeddings, build_label_encoder, glove_to_tensor, split_dataset\n",
    "from data_augmentation import DataAugmentation\n",
    "\n",
    "# Parameters\n",
    "DATASET_NAME = \"imdb\" # imdb / sentiment_data_custom\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "EMBEDDING_DIM = 50\n",
    "HIDDEN_DIM = 128\n",
    "OUTPUT_DIM = 1\n",
    "TRAIN_SPLIT_RATIO = 0.8\n",
    "VAL_SPLIT_RATIO = 0.1\n",
    "TEST_SPLIT_RATIO = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import disable_caching\n",
    "\n",
    "disable_caching()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings\n",
    "\n",
    "We will use the pre-trained GloVe embeddings for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe embeddings\n",
    "embedding = load_glove_embeddings(\"C:\\Users\\notebook\\Documents\\nlp-training-REL_20230706\\notebooks\\data\\embeddings\\glove.6B.50d.txt\")\n",
    "label_encoder = build_label_encoder(vocabs=list(embedding.keys()))\n",
    "embedding_tensor = glove_to_tensor(label_encoder, embedding, embedding_dim=EMBEDDING_DIM)\n",
    "\n",
    "# Load tokenizer for T5\n",
    "pretrained_name = \"t5-small\"\n",
    "tokenizer_t5 = T5Tokenizer.from_pretrained(pretrained_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation with EDA (Easy Data Augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the IMDB dataset\n",
    "raw_data = load_data(name=DATASET_NAME, split=\"train[:10]+train[-10:]\")\n",
    "raw_data.cleanup_cache_files()\n",
    "\n",
    "# Split data into training, validation, and test sets\n",
    "data = split_dataset(raw_data, train_ratio=TRAIN_SPLIT_RATIO, validation_ratio=VAL_SPLIT_RATIO, test_ratio=TEST_SPLIT_RATIO)\n",
    "\n",
    "# Execute EDA on data and preprocess data\n",
    "for data_split in [\"train\", \"val\", \"test\"]:\n",
    "    data_augmentation = DataAugmentation(data[data_split])\n",
    "    data[data_split] = data_augmentation.eda(\n",
    "        alpha_sr=0.1, alpha_ri=0.1, alpha_rs=0.1, p_rd=0.1, num_aug=1\n",
    "    )\n",
    "    data[data_split] = preprocess_data(data[data_split], label_encoder, embedding_tensor, tokenizer_t5)\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader = create_dataloader(data[\"train\"], batch_size=BATCH_SIZE)\n",
    "val_dataloader = create_dataloader(data[\"val\"], batch_size=BATCH_SIZE)\n",
    "test_dataloader = create_dataloader(data[\"test\"], batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain and Evaluate Models on EDA Augmented Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LSTM model\n",
    "lstm = LSTMSentiment(\n",
    "    embedding_tensor=embedding_tensor,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    output_dim=OUTPUT_DIM,\n",
    ")\n",
    "\n",
    "# Train LSTM\n",
    "optimizer = optim.Adam(lstm.parameters(), lr=1e-3)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "lstm.train_loop(train_dataloader, val_dataloader, optimizer, criterion, epochs=EPOCHS)\n",
    "\n",
    "# Evaluate LSTM\n",
    "lstm_result = lstm.evaluate_loop(test_dataloader, criterion)\n",
    "print(f\"LSTM accuracy: {lstm_result['accuracy']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FILTERS = 3\n",
    "FILTER_SIZES = [3, 4, 5]\n",
    "\n",
    "# Initialize CNN model\n",
    "cnn = CNNSentiment(\n",
    "    embedding_tensor=embedding_tensor,\n",
    "    num_filters=NUM_FILTERS,\n",
    "    filter_sizes=FILTER_SIZES,\n",
    "    output_dim=OUTPUT_DIM,\n",
    ")\n",
    "\n",
    "# Train CNN\n",
    "optimizer = optim.Adam(cnn.parameters(), lr=1e-3)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "cnn.train_loop(train_dataloader, val_dataloader, optimizer, criterion, epochs=EPOCHS)\n",
    "\n",
    "# Evaluate CNN\n",
    "cnn_result = cnn.evaluate_loop(test_dataloader, criterion)\n",
    "print(f\"CNN accuracy: {cnn_result['accuracy']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize T5 model according to your tokenizer and pre-trained model\n",
    "t5 = T5Sentiment(pretrained_name)\n",
    "optimizer = optim.AdamW(t5.parameters(), lr=5e-5)\n",
    "\n",
    "# Train T5\n",
    "t5.train_loop(train_dataloader, val_dataloader, optimizer, epochs=EPOCHS)\n",
    "\n",
    "# Evaluate T5\n",
    "t5_result = t5.evaluate_loop(test_dataloader)\n",
    "print(f\"T5 accuracy: {t5_result['accuracy']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LR model\n",
    "lr = LRSentiment()\n",
    "data = data.with_format(\"np\")\n",
    "\n",
    "# Train LR\n",
    "lr.train(data[\"train\"][\"input_glove_vectors\"], data[\"train\"][\"label\"])\n",
    "\n",
    "# Evaluate LR\n",
    "lr_result = lr.evaluate(data[\"test\"][\"input_glove_vectors\"], data[\"test\"][\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SVM model\n",
    "svm = SVMSentiment()\n",
    "\n",
    "# Train SVM\n",
    "svm.train(data[\"train\"][\"input_glove_vectors\"], data[\"train\"][\"label\"])\n",
    "\n",
    "# Evaluate SVM\n",
    "svm_result = svm.evaluate(data[\"test\"][\"input_glove_vectors\"], data[\"test\"][\"label\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RFT model\n",
    "rft = RFTSentiment()\n",
    "\n",
    "# Train RFT\n",
    "rft.train(data[\"train\"][\"input_glove_vectors\"], data[\"train\"][\"label\"])\n",
    "\n",
    "# Evaluate RFT\n",
    "rft_result = rft.evaluate(data[\"test\"][\"input_glove_vectors\"], data[\"test\"][\"label\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"model\", \"accuracy\", \"f1\", \"precision\", \"recall\"]\n",
    "\n",
    "results = [lstm_result, cnn_result, t5_result, lr_result, svm_result, rft_result]\n",
    "final_result = pd.DataFrame(results)\n",
    "\n",
    "final_result[\"train_ratio\"] = TRAIN_SPLIT_RATIO\n",
    "final_result[\"val_ratio\"] = VAL_SPLIT_RATIO\n",
    "final_result[\"train_ratio\"] = TRAIN_SPLIT_RATIO\n",
    "\n",
    "final_result[cols]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "495b914dd0dc17102080dee1e2e6b4843659ba27b4499bb2c586bc0e639f5b10"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
