{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis on Reviews\n",
    "\n",
    "In this project, we will use machine learning algorithms and deep learning models with different word embedding approaches for predicting the sentiment of reviews. We will use the following models and compare their performance on different datasets: LSTM, CNN, T5, LR, SVM, and RFT.\n",
    "\n",
    "Data sources\n",
    "* IMBD dataset\n",
    "  * https://huggingface.co/datasets/imdb\n",
    "* Word embedding\n",
    "  * GloVe - https://nlp.stanford.edu/projects/glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "In this section, we will set up all the necessary parameters and import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "from lstm import LSTMSentiment\n",
    "from cnn import CNNSentiment\n",
    "from t5_model import T5Sentiment\n",
    "from lr import LRSentiment\n",
    "from svm import SVMSentiment\n",
    "from rft import RFTSentiment\n",
    "from utils import load_data, preprocess_data, create_dataloader, load_glove_embeddings, build_label_encoder, glove_to_tensor, split_dataset\n",
    "from data_augmentation import DataAugmentation\n",
    "\n",
    "# Parameters\n",
    "DATASET_NAME = \"imdb\"\n",
    "#pd.read_csv('/Users/notebook/Documents/nlp-training-REL_20230706/notebooks/data/IMDB Dataset.csv')\n",
    "\n",
    "EPOCHS = 10\n",
    "EMBEDDING_DIM = 50\n",
    "HIDDEN_DIM = 128\n",
    "OUTPUT_DIM = 1\n",
    "TRAIN_SPLIT_RATIO = 0.8\n",
    "VAL_SPLIT_RATIO = 0.1\n",
    "TEST_SPLIT_RATIO = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import disable_caching\n",
    "\n",
    "disable_caching()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings\n",
    "\n",
    "We will use the pre-trained GloVe embeddings for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load GloVe embeddings\n",
    "embedding = load_glove_embeddings(\"data/embeddings/glove.6B.50d.txt\")\n",
    "label_encoder = build_label_encoder(vocabs=list(embedding.keys()))\n",
    "embedding_tensor = glove_to_tensor(label_encoder, embedding, embedding_dim=EMBEDDING_DIM)\n",
    "\n",
    "# Load tokenizer for T5\n",
    "pretrained_name = \"t5-small\"\n",
    "tokenizer_t5 = T5Tokenizer.from_pretrained(pretrained_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(DATASET_NAME\u001b[38;5;241m.\u001b[39mhead())\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "print(DATASET_NAME.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (C:/Users/notebook/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/80 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the IMDB dataset\n",
    "data = load_data(name=DATASET_NAME, split=\"train[:50]+train[-50:]\")\n",
    "data.cleanup_cache_files()\n",
    "\n",
    "# Split data into training, validation, and test sets\n",
    "data = split_dataset(data, train_ratio=TRAIN_SPLIT_RATIO, validation_ratio=VAL_SPLIT_RATIO, test_ratio=TEST_SPLIT_RATIO)\n",
    "\n",
    "# Preprocess data\n",
    "data = preprocess_data(data, label_encoder, embedding_tensor, tokenizer_t5)\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader = create_dataloader(data[\"train\"], batch_size=16)\n",
    "val_dataloader = create_dataloader(data[\"val\"], batch_size=16)\n",
    "test_dataloader = create_dataloader(data[\"test\"], batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\notebook\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Train loss: 0.690 | Train acc: 56.25% | Validation loss: 0.761 | Validation acc: 20.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\notebook\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Train loss: 0.682 | Train acc: 56.25% | Validation loss: 0.805 | Validation acc: 20.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\notebook\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Train loss: 0.672 | Train acc: 56.25% | Validation loss: 0.824 | Validation acc: 20.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\notebook\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | Train loss: 0.662 | Train acc: 57.50% | Validation loss: 0.823 | Validation acc: 20.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\notebook\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | Train loss: 0.660 | Train acc: 58.75% | Validation loss: 0.765 | Validation acc: 20.00%\n",
      "Epoch: 6 | Train loss: 0.631 | Train acc: 76.25% | Validation loss: 0.809 | Validation acc: 30.00%\n",
      "Epoch: 7 | Train loss: 0.533 | Train acc: 72.50% | Validation loss: 0.667 | Validation acc: 90.00%\n",
      "Epoch: 8 | Train loss: 0.509 | Train acc: 77.50% | Validation loss: 0.606 | Validation acc: 70.00%\n",
      "Epoch: 9 | Train loss: 0.457 | Train acc: 76.25% | Validation loss: 0.799 | Validation acc: 60.00%\n",
      "Epoch: 10 | Train loss: 0.371 | Train acc: 85.00% | Validation loss: 0.466 | Validation acc: 80.00%\n",
      "LSTM accuracy: 0.7\n"
     ]
    }
   ],
   "source": [
    "# Initialize LSTM model\n",
    "lstm = LSTMSentiment(\n",
    "    embedding_tensor=embedding_tensor,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    output_dim=OUTPUT_DIM,\n",
    ")\n",
    "\n",
    "# Train LSTM\n",
    "optimizer = optim.Adam(lstm.parameters(), lr=1e-3)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "lstm.train_loop(train_dataloader, val_dataloader, optimizer, criterion, epochs=EPOCHS)\n",
    "\n",
    "# Evaluate LSTM\n",
    "lstm_result = lstm.evaluate_loop(test_dataloader, criterion)\n",
    "print(f\"LSTM accuracy: {lstm_result['accuracy']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\notebook\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Train loss: 0.682 | Train acc: 55.00% | Validation loss: 0.776 | Validation acc: 20.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\notebook\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Train loss: 0.661 | Train acc: 57.50% | Validation loss: 0.773 | Validation acc: 20.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\notebook\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Train loss: 0.654 | Train acc: 60.00% | Validation loss: 0.771 | Validation acc: 20.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\notebook\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | Train loss: 0.632 | Train acc: 60.00% | Validation loss: 0.769 | Validation acc: 20.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\notebook\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | Train loss: 0.676 | Train acc: 56.25% | Validation loss: 0.771 | Validation acc: 20.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\notebook\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 | Train loss: 0.623 | Train acc: 66.25% | Validation loss: 0.771 | Validation acc: 20.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\notebook\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 | Train loss: 0.629 | Train acc: 62.50% | Validation loss: 0.766 | Validation acc: 20.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\notebook\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 | Train loss: 0.634 | Train acc: 71.25% | Validation loss: 0.765 | Validation acc: 20.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\notebook\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 | Train loss: 0.629 | Train acc: 63.75% | Validation loss: 0.769 | Validation acc: 20.00%\n",
      "Epoch: 10 | Train loss: 0.620 | Train acc: 65.00% | Validation loss: 0.775 | Validation acc: 20.00%\n",
      "CNN accuracy: 0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\notebook\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\notebook\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "NUM_FILTERS = 3\n",
    "FILTER_SIZES = [3, 4, 5]\n",
    "\n",
    "# Initialize CNN model\n",
    "cnn = CNNSentiment(\n",
    "    embedding_tensor=embedding_tensor,\n",
    "    num_filters=NUM_FILTERS,\n",
    "    filter_sizes=FILTER_SIZES,\n",
    "    output_dim=OUTPUT_DIM,\n",
    ")\n",
    "\n",
    "# Train CNN\n",
    "optimizer = optim.Adam(cnn.parameters(), lr=1e-3)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "cnn.train_loop(train_dataloader, val_dataloader, optimizer, criterion, epochs=EPOCHS)\n",
    "\n",
    "# Evaluate CNN\n",
    "cnn_result = cnn.evaluate_loop(test_dataloader, criterion)\n",
    "print(f\"CNN accuracy: {cnn_result['accuracy']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize T5 model according to your tokenizer and pre-trained model\n",
    "t5 = T5Sentiment(pretrained_name)\n",
    "optimizer = optim.AdamW(t5.parameters(), lr=5e-5)\n",
    "\n",
    "# Train T5\n",
    "t5.train_loop(train_dataloader, val_dataloader, optimizer, epochs=EPOCHS)\n",
    "\n",
    "# Evaluate T5\n",
    "t5_result = t5.evaluate_loop(test_dataloader)\n",
    "print(f\"T5 accuracy: {t5_result['accuracy']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LR model\n",
    "lr = LRSentiment()\n",
    "data = data.with_format(\"np\")\n",
    "\n",
    "# Train LR\n",
    "lr.train(data[\"train\"][\"input_glove_vectors\"], data[\"train\"][\"label\"])\n",
    "\n",
    "# Evaluate LR\n",
    "lr_result = lr.evaluate(data[\"test\"][\"input_glove_vectors\"], data[\"test\"][\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SVM model\n",
    "svm = SVMSentiment()\n",
    "\n",
    "# Train SVM\n",
    "svm.train(data[\"train\"][\"input_glove_vectors\"], data[\"train\"][\"label\"])\n",
    "\n",
    "# Evaluate SVM\n",
    "svm_result = svm.evaluate(data[\"test\"][\"input_glove_vectors\"], data[\"test\"][\"label\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RFT model\n",
    "rft = RFTSentiment()\n",
    "\n",
    "# Train RFT\n",
    "rft.train(data[\"train\"][\"input_glove_vectors\"], data[\"train\"][\"label\"])\n",
    "\n",
    "# Evaluate RFT\n",
    "rft_result = rft.evaluate(data[\"test\"][\"input_glove_vectors\"], data[\"test\"][\"label\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"model\", \"accuracy\", \"f1\", \"precision\", \"recall\"]\n",
    "\n",
    "results = [lstm_result, cnn_result, t5_result, lr_result, svm_result, rft_result]\n",
    "final_result = pd.DataFrame(results)\n",
    "\n",
    "final_result[\"train_ratio\"] = TRAIN_SPLIT_RATIO\n",
    "final_result[\"val_ratio\"] = VAL_SPLIT_RATIO\n",
    "final_result[\"train_ratio\"] = TRAIN_SPLIT_RATIO\n",
    "\n",
    "final_result[cols]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "495b914dd0dc17102080dee1e2e6b4843659ba27b4499bb2c586bc0e639f5b10"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
